{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e3923b-98e3-421f-a9a1-d6ed09264e13",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with Unstructured Data using GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31fb1f8",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Using the command line, create a new Conda environment using the `environment.yml` file:\n",
    "```bash\n",
    "module load miniforge\n",
    "conda env create -f environment.yml\n",
    "conda activate rag_ollama\n",
    "```\n",
    "\n",
    "Alternatively, install the necessary packages manually:\n",
    "\n",
    "```bash\n",
    "module load miniforge\n",
    "conda create -n rag_ollama jupyterlab langchain-ollama langchain-chroma langchain-community\n",
    "conda activate rag_ollama\n",
    "pip install \"unstructured[pdf]\"\n",
    "```\n",
    "\n",
    "Create a Jupyter kernel for your environment:\n",
    "```bash\n",
    "python -m ipykernel install --user --name rag_ollama\n",
    "```\n",
    "\n",
    "Connect this notebook to the Jupyter kernel you just created. You may need to disconnect from and reconnect to your Jupyter session.\n",
    "\n",
    "Run the setup script to start Ollama and download the embedding and language models:\n",
    "```bash\n",
    "sh start_ollama.sh\n",
    "```\n",
    "\n",
    "Import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "from langchain.agents import create_agent\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3425058e",
   "metadata": {},
   "source": [
    "Set environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7156e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "EMBEDDING_MODEL_NAME = config[\"embedding_model\"]\n",
    "LLM_NAME = config[\"llm\"]\n",
    "\n",
    "DOCS_PATH = os.path.join(os.getcwd(), \"docs\")\n",
    "VECTOR_STORE_PATH = os.path.join(os.getcwd(), \"vector_store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87272f73",
   "metadata": {},
   "source": [
    "Initialize embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(\n",
    "    model=EMBEDDING_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bef930",
   "metadata": {},
   "source": [
    "## Processing PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b8f3f-1847-4bd8-87a9-098913b93e72",
   "metadata": {},
   "source": [
    "We have a set of PDFs that we would like to input into our RAG pipeline. We cannot do this directly, however. While PDFs are optimized for humans to read and comprehend, machines have a harder time. So, we must first process our documents so that they can be efficiently searched by a computer. We will do this in two steps:\n",
    "1. Extract the raw text from the PDFs\n",
    "2. Convert the text into vectors using an embedding model\n",
    "\n",
    "### Extracting Text from PDFs\n",
    "\n",
    "The `unstructured` software has a PDF loading tool that extracts text from PDFs and ignores images. This software uses the `pdfminer.six` Python package under the hood, which is very popular for reading PDFs using Python.\n",
    "\n",
    "*Note: `unstructured` has loaders for other file formats as well, such as Markdown or Word documents.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbbddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mute pdfminer warnings globally\n",
    "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pdfminer.pdffont\").setLevel(logging.ERROR)\n",
    "\n",
    "def load_documents(docs_path):\n",
    "    \"\"\"\n",
    "    Load documents from the specified directory recursively. Documents must be\n",
    "    in .pdf format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the documents recursively:\n",
    "    documents = []\n",
    "    for file_name in os.listdir(docs_path):\n",
    "        file_path = os.path.join(docs_path, file_name)\n",
    "        if file_name.endswith('.pdf'):\n",
    "            loader = UnstructuredPDFLoader(file_path, languages=[\"eng\"])\n",
    "            doc = loader.load()\n",
    "            doc[0].metadata[\"source\"] = file_name\n",
    "            documents.extend(doc)\n",
    "        elif os.path.isdir(file_path):\n",
    "            documents.extend(load_documents(file_path))\n",
    "    return documents\n",
    "\n",
    "documents = load_documents(DOCS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd5496c",
   "metadata": {},
   "source": [
    "This creates a list of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dfe2b5-dffb-4989-9887-34db59320da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f515a23-e265-4cc2-8502-ff4fe0c75699",
   "metadata": {},
   "source": [
    "Each document object contains metadata, such as the document title, as well as the raw text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab1399e-9c12-4ab4-9554-b86b4aac2d00",
   "metadata": {},
   "source": [
    "### Creating a Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d5cf8f-f132-459b-aed9-fc4825c8fd42",
   "metadata": {},
   "source": [
    "Now that we have extracted the text from the PDFs, we must further process our data so that it can be efficiently searched by our pipeline. We will do this by saving our documents into a **vector store**.\n",
    "\n",
    "#### Chunking\n",
    "\n",
    "The vectors will be created using an embedding model, but before we do this, we must **chunk** our documents. We have to do this because embedding models have a context limit, and some of our documents are too large to fit into a single vector. For example, the embedding model that we're using, `bge-m3`, has a context limit of 8,000 tokens (per its [datasheet](https://ollama.com/library/bge-m3)).\n",
    "\n",
    "How you chunk your documents is important because each chunk should represent a coherent idea that reflects the intended meaning from the original document. If your chunks are too large, you risk feeding your pipeline unnecessary or only tangentially relevant information. If your chunks are too small, then you may lose essential context that helps the retriever and model understand what a chunk is actually about. Consider this example:\n",
    "\n",
    "> Red squirrels have a varied and adaptable diet that changes with the seasons. They primarily eat seeds from conifer cones, such as pine, spruce, and fir, carefully stripping the cones to reach the nutritious seeds inside. In addition to seeds, they consume nuts, berries, fruits, buds, and fungi, especially mushrooms. Red squirrels are also known to occasionally eat insects, bird eggs, or nestlings when plant food is scarce.\n",
    "\n",
    "> Red squirrels typically live in forests dominated by coniferous or mixed trees, which provide both food and shelter. They build nests, called dreys, high in the trees using twigs, leaves, moss, and bark for insulation. Some individuals also use hollow trees or abandoned woodpecker holes for nesting. Their habitat usually includes well-defined territories that they actively defend from other squirrels.\n",
    "\n",
    "If we combine both paragraphs into a single chunk, then a query about the diet of red squirrels will retrieve information about their habitat and nesting behavior as well. While this information is related, it is not directly relevant to the question being asked. As a result, the retrieved context may fill up the model’s context window more quickly and crowd out other, more relevant chunks from different documents.\n",
    "\n",
    "On the other hand, if we split the document too aggressively (e.g., by making each sentence into its own chunk), then the sentences' original context is lost. Important information that is implicit in the surrounding sentences may no longer be available to the retriever. For example, if a user asks, \"What do red squirrels eat?\", the retriever may fail to identify the following sentence as relevant:\n",
    "\n",
    "> They primarily eat seeds from conifer cones, such as pine, spruce, and fir, carefully stripping the cones to reach the nutritious seeds inside.\n",
    "\n",
    "On its own, this sentence does not explicitly mention red squirrels. Without the surrounding context, the retriever (and the model) has no clear signal that the sentence is describing the diet of red squirrels rather than some other animal.\n",
    "\n",
    "In this example, the best method would be to treat each paragraph as its own chunk, as each paragraph has a distinct topic.\n",
    "\n",
    "Of course, we cannot manually chunk every document. Instead, chunking tools allow us to specify chunk sizes, and also include chunk overlaps, which help avoid context loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e005b51b-f1c3-4494-a058-f84c79f49502",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10000\n",
    "chunk_overlap = int(.2 * chunk_size)\n",
    "separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, # Given in characters, not tokens (1 token = 3-4 characters)\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    "    separators=separators,\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "print(f\"Loaded {len(documents)} docs -> split into {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5813b4f8-bdfc-47d5-891d-a7631247dda5",
   "metadata": {},
   "source": [
    "After splitting, we have multiple documents, each representing a different chunk of each source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc6f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in split_docs:\n",
    "    print(doc.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11662705-b707-491f-8971-8ecb1026205e",
   "metadata": {},
   "source": [
    "Because we used overlapping, the documents are not mutually exclusive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891e5c7-6cca-4b3b-9229-21bf565d0d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Document 0:\\n\")\n",
    "print(split_docs[0].page_content[-chunk_overlap:])\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"Document 1:\\n\")\n",
    "print(split_docs[1].page_content[:chunk_overlap])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b031e72",
   "metadata": {},
   "source": [
    "#### Converting Chunks into Vectors\n",
    "\n",
    "The final step of data preparation is to convert our documents into \"vectors,\" which are numerical representations that capture the meaning of words, sentences, and passages. This allows the pipeline to efficiently run a similarity search with queries to extract contextually relevant documents.\n",
    "\n",
    "*How do we choose an embedding model?*\n",
    "\n",
    "The embedded documents get saved to a Chroma database (\"vector store\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Chroma.from_documents(documents=split_docs,\n",
    "                      embedding=embeddings,\n",
    "                      persist_directory=VECTOR_STORE_PATH) # Specifying persist_directory saves the vector store as a file so we don't have to recreate it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae4005",
   "metadata": {},
   "source": [
    "Once the vector store has been saved to a file, you can read it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0f3f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(persist_directory=VECTOR_STORE_PATH,\n",
    "                      embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6910fc6a-4753-4cbd-a4b2-adbbf51b9bfb",
   "metadata": {},
   "source": [
    "Now that we have a vector store, we can evaluate its ability to retrieve relevant information using similarity searches. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_docs(query, vector_store):\n",
    "    results = vector_store.similarity_search_with_score(\n",
    "        query, k=5\n",
    "    )\n",
    "    for res, score in results:\n",
    "        print(res.metadata[\"source\"], f\"({round(score, 2)})\")\n",
    "\n",
    "query = \"How does BBR work?\"\n",
    "get_relevant_docs(query, vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce66aed-09ce-4450-80d4-47c45688127e",
   "metadata": {},
   "source": [
    "Using a basic query, our retriever seems to work well. Let's try something more complex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf4f77-52c1-4112-a52c-e68f429b473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Contrast the strategies used by CUBIC, Hybla, and BBR to handle connections with long Round Trip Times (RTT).\"\n",
    "get_relevant_docs(query, vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0bcdee-2626-413d-8bea-eddd61d5e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do 'loss-based' protocols and 'delay-based' protocols struggle specifically in the context of Low-Earth-Orbit (LEO) satellite networks?\"\n",
    "get_relevant_docs(query, vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec85ce-5945-4060-85c0-8fb260280fff",
   "metadata": {},
   "source": [
    "## Running RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3f1d81",
   "metadata": {},
   "source": [
    "### Run an LLM locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e7c55-9365-4734-8180-0ef152d5c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=LLM_NAME, temperature=0.5)\n",
    "display(Markdown(llm.invoke(\"Where is MIT located?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb135986-c1ae-4b25-ba4d-0437a22fd7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(llm.invoke(\"How do I use Pandas to read a Parquet file in Python?\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be523553",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Set up the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b4783",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query, k=3)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    # Print documents used:\n",
    "    print(\"\\nRetrieved documents:\") ##\n",
    "    for doc in retrieved_docs: ##\n",
    "        print(doc.metadata[\"source\"]) ##\n",
    "    print()\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Answer only using the information from the following documents.\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "\n",
    "agent = create_agent(llm, tools=[], middleware=[prompt_with_context])\n",
    "\n",
    "def pose(query):\n",
    "    for step in agent.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        display(Markdown(step[\"messages\"][-1].text))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf177d3f-75bc-4b28-b247-a8dc5cd4bb6c",
   "metadata": {},
   "source": [
    "Testing questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd7b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do the design philosophies of BBR, CUBIC, and NewReno differ in their interpretation of network 'signals'?\"\n",
    "pose(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d59d090-19b6-4be7-a8c5-15f20ff4d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Why does TCP Vegas perform poorly in LEO satellite networks compared to BBR?\"\n",
    "pose(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b886af2-6136-48cc-bc95-387a3d178d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Compare how TCP CUBIC and TCP Hybla address the problem of 'RTT Unfairness.'\"\n",
    "pose(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47f5c2-a5f8-4e7a-a8a8-9ba7d709277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the significance of the 'recover' variable and 'Partial ACKs' in the NewReno algorithm?\"\n",
    "pose(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4094c912-ade1-46e4-b38c-9ca344f2f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain the sequential relationship between BBR’s 'Startup' and 'Drain' states.\"\n",
    "pose(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c6bd1-9621-49eb-8043-90a1de61f179",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "How do we know that these answers are coming from the documents or are from the model's pre-trained knowledge? This takes a bit of prompt engineering. Let's try editing the system message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882b0df-1e0d-4c68-961e-e04dfd85448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query, k=3)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    # Print documents used:\n",
    "    print(\"\\nRetrieved documents:\") ##\n",
    "    for doc in retrieved_docs: ##\n",
    "        print(doc.metadata[\"source\"]) ##\n",
    "    print()\n",
    "\n",
    "    system_message = (\n",
    "        # Edit here:\n",
    "        \"You are a helpful assistant. Answer only using the information from the following documents. If the documents do not answer the question, please say so.\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "\n",
    "agent = create_agent(llm, tools=[], middleware=[prompt_with_context])\n",
    "\n",
    "def pose(query):\n",
    "    for step in agent.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        display(Markdown(step[\"messages\"][-1].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a09893-a6aa-4c53-8b2b-aadd49a103e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who was the fifth president of the United States?\"\n",
    "pose(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_ollama",
   "language": "python",
   "name": "rag_ollama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
