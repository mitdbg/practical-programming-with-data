{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study: Evaluating Congestion Control for Satellite Networks\n",
        "\n",
        "### Scenario:\n",
        "Your lab is working on a project simulating congestion control protocols on satellite networks. They have run some simulations and produced data stored in Github. They have written some scripts to help with creating plots, but there are some issues they want you to take a look at because they heard you are taking ***Programming With Data*** as part of ***PCT***. Follow the tasks below and help your lab make good decisions.\n",
        "\n",
        "You donâ€™t need any prior knowledge about satellite networks or congestion control protocols to complete any of the examples.\n",
        "\n",
        "#### Task\n",
        "Look through `plot_multi_throughput.py` script and understand what is being done\n",
        "\n",
        "#### Task\n",
        "Use `time_scripts.py` to see the example run times when using csv vs parquet\n",
        "\n",
        "#### Task\n",
        "Decide which file format you want to use when running `generate_plots.sh`\n"
      ],
      "metadata": {
        "id": "FhJxW7lgXpmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Download the Data"
      ],
      "metadata": {
        "id": "IKfFT805Y-XO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfQOF-tiU-EX"
      },
      "outputs": [],
      "source": [
        "# download and extract tar file\n",
        "!wget https://raw.githubusercontent.com/mitdbg/practical-programming-with-data/main/2026/day1_materials/sim_logs.tar.gz\n",
        "!tar -xzf sim_logs.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# examine the extracted files\n",
        "!ls sim_logs"
      ],
      "metadata": {
        "id": "ayz9CdE6aq9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2:\n",
        "Somebody suggested to your labmate that they use the `parquet` file format to store the large logs produced by the simulations. Find out why they may have made this suggestion by looking at some differences in the performance of `parquet` vs `csv`."
      ],
      "metadata": {
        "id": "atGRdu1ab0Cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read parquet file\n",
        "df = pd.read_parquet(\"sim_logs/LeoSharedPath-1ms-TcpBbr-throughput.parquet\")\n",
        "\n",
        "# save data as csv\n",
        "df.to_csv(\"sim_logs/LeoSharedPath-1ms-TcpBbr-throughput.csv\", index=False)"
      ],
      "metadata": {
        "id": "J40lCZC5clXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first, let's look at the difference in the files' raw size(s)\n",
        "!ls -lth sim_logs/ | grep LeoSharedPath-1ms-TcpBbr-throughput"
      ],
      "metadata": {
        "id": "9PI18WUzbszC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next, let's time how long it takes for us to read each file from disk\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "df = pd.read_parquet(\"sim_logs/LeoSharedPath-1ms-TcpBbr-throughput.parquet\")\n",
        "parquet_read_time = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "df = pd.read_csv(\"sim_logs/LeoSharedPath-1ms-TcpBbr-throughput.csv\")\n",
        "csv_read_time = time.time() - start_time\n",
        "\n",
        "print(f\"parquet read time: {parquet_read_time}\")\n",
        "print(f\"csv read time: {csv_read_time}\")\n",
        "print(\"---\")\n",
        "print(f\"speedup: {csv_read_time / parquet_read_time:.2f}x\")"
      ],
      "metadata": {
        "id": "S4Y8FaqsmwUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's examine the first few rows of the dataframe\n",
        "df.head()"
      ],
      "metadata": {
        "id": "MCv3q6hqmy7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parquet enables you to only read / load certain columns from the raw data;\n",
        "# as a final experiment, let's see how long it takes us to load the flowId and bytes\n",
        "# columns with parquet, relative to loading the entire CSV and post-filtering it\n",
        "# next, let's time how long it takes for us to read each file from disk\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "df = pd.read_parquet(\"sim_logs/LeoSharedPath-1ms-TcpBbr-throughput.parquet\", columns=[\"flowId\", \"bytes\"])\n",
        "parquet_read_time = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "df = pd.read_csv(\"sim_logs/LeoSharedPath-1ms-TcpBbr-throughput.csv\")\n",
        "df = df.loc[:, [\"flowId\", \"bytes\"]]\n",
        "csv_read_time = time.time() - start_time\n",
        "\n",
        "print(f\"parquet read time: {parquet_read_time}\")\n",
        "print(f\"csv read time: {csv_read_time}\")\n",
        "print(\"---\")\n",
        "print(f\"speedup: {csv_read_time / parquet_read_time:.2f}\")"
      ],
      "metadata": {
        "id": "kRZ5yAp_m80O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3:\n",
        "Let's examine the queueing delay associated with the BBR algorithm (`TcpBbr`) in the `GsToGs` setting."
      ],
      "metadata": {
        "id": "NiSMRij6dIWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delay_df = pd.read_parquet(\"sim_logs/LeoGsToGs-1ms-TcpBbr-delay.parquet\")"
      ],
      "metadata": {
        "id": "C7QiMF27eceT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delay_df.head()"
      ],
      "metadata": {
        "id": "486IYw0pe9TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: How many unique flows are in this data sample?"
      ],
      "metadata": {
        "id": "WDm5vBOgfFL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Cleaning: There's a typo in the column name \"queue_dealy(us)\" rename this column to \"queue_delay(us)\""
      ],
      "metadata": {
        "id": "tvZC9vTfgSgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting: plot the queueing delay in **milliseconds** vs. time in seconds"
      ],
      "metadata": {
        "id": "LMq0a2DJgkqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting: to better examine the queueing delay behavior, generate another\n",
        "# version of the plot which does not contain samples with >1.0 ms of delay"
      ],
      "metadata": {
        "id": "Rm3WMt40gTlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4:\n",
        "\n",
        "Let's now examine the congestion window of various congestion control algorithms for the `GsToGs` setting."
      ],
      "metadata": {
        "id": "aw844lbmjYgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the congestion window (cwnd) data for `TcpBbr`, `TcpCubic`, `TcpNewReno`, and `TcpHybla` in the `GsToGs` setting\n",
        "bbr_df = pd.read_parquet(\"sim_logs/LeoGsToGs-1ms-TcpBbr-cwnd.parquet\")\n",
        "cubic_df = pd.read_parquet(\"sim_logs/LeoGsToGs-1ms-TcpCubic-cwnd.parquet\")\n",
        "reno_df = pd.read_parquet(\"sim_logs/LeoGsToGs-1ms-TcpNewReno-cwnd.parquet\")\n",
        "hybla_df = pd.read_parquet(\"sim_logs/LeoGsToGs-1ms-TcpHybla-cwnd.parquet\")"
      ],
      "metadata": {
        "id": "tNasIIm5b9eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's examine the head of each dataframe to ensure their schemas match\n",
        "print(bbr_df.head())\n",
        "print(\"---\")\n",
        "print(cubic_df.head())\n",
        "print(\"---\")\n",
        "print(reno_df.head())\n",
        "print(\"---\")\n",
        "print(hybla_df.head())"
      ],
      "metadata": {
        "id": "qStkwzgo0ywt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task: coalesce all the data into a single dataframe, adding a new column 'cc_algo',\n",
        "# which stores the name of the congestion control algorithm associated with that data;"
      ],
      "metadata": {
        "id": "Jh3--Ybp0riI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task: Remove outliers by filtering for congestion windows < 3e5"
      ],
      "metadata": {
        "id": "LMweforw016m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting: plot the congestion window as a function of time for each algorithm;\n",
        "# plot each algorithm as a separate line on the same plot w/colors and a legend to distinguish them"
      ],
      "metadata": {
        "id": "zE5pp-kZ05jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task: bucket the data into 10s time intervals, write a function which computes time(s) // 10\n",
        "# and apply that function to the time(s) column, saving the output in a new \"interval\" column"
      ],
      "metadata": {
        "id": "6W4CiB-w07eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task: compute and print the mean congestion window size for each unique (cc_algo, interval) (Hint: use .groupby())"
      ],
      "metadata": {
        "id": "ejaud1cE1Jw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task: in how many intervals does TcpCubic have the largest mean congestion window?\n",
        "#\n",
        "# Hint:\n",
        "# - Initialize two empty lists, one to contain data for TcpCubic and another to contain data for the other algorithms\n",
        "# - Use the same groupby from before to compute the mean congestion window for each algorithm in each interval\n",
        "# - Put each interval and congestion window into a dictionary:\n",
        "#   - for cubic data: {\"interval\": interval, \"mean_cwnd_cubic\": group_df['cwnd'].mean()}\n",
        "#   - for the rest: {\"interval\": interval, \"mean_cwnd\": group_df['cwnd'].mean()}\n",
        "#   - append the dictionary to its respective list (cubic data --> cubic list, other data --> other list)\n",
        "# - Construct a dataframe from each list\n",
        "# - Join the dataframes on the \"interval\" column (use .merge()) and store the output in a final dataframe\n",
        "# - Group the final by interval, and compute the fraction of intervals with mean_cwnd_cubic > mean_cwnd for all rows"
      ],
      "metadata": {
        "id": "SaJMe9ts1NhN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}